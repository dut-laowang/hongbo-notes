[
  {
    "objectID": "ppt.html",
    "href": "ppt.html",
    "title": "My PPT Showcase (Click to view PDFs)",
    "section": "",
    "text": "03/15/2025 PPT for ICASSP 2025 Video\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n03/13/2025 Survey on Knowledge Distillation Techniques (LLMs)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n03/02/2025 Survey on DeepSeek V3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n01/21/2025 PhD Application PPT Slides (UTokyo)"
  },
  {
    "objectID": "paper4.html",
    "href": "paper4.html",
    "title": "Study Notes",
    "section": "",
    "text": "Reinforcement LearningMultimodal PCL\n\n\n\n\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.plot(np.random.randn(100).cumsum())\nplt.title(\"RL Noise Simulation\")\nplt.show()\n\nConclusion: RL model adapts well under noisy settings.\n\n\n\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nrewards = np.linspace(0, 10, 100)\nshaped = rewards ** 0.5\nprint(\"Shaped Rewards:\", shaped[:5])\n\nConclusion: Adaptive reward shaping speeds up training.\n\n\n\n\n\nMultimodal PCL detection using video and text features.\n\ntext_features = [0.8, 0.9, 0.85]\nvideo_features = [0.7, 0.75, 0.78]\ncombined = [ (t+v)/2 for t, v in zip(text_features, video_features)]\nprint(\"Combined Features:\", combined)"
  },
  {
    "objectID": "paper4.html#research-updates",
    "href": "paper4.html#research-updates",
    "title": "Study Notes",
    "section": "",
    "text": "Reinforcement LearningMultimodal PCL\n\n\n\n\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.plot(np.random.randn(100).cumsum())\nplt.title(\"RL Noise Simulation\")\nplt.show()\n\nConclusion: RL model adapts well under noisy settings.\n\n\n\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nrewards = np.linspace(0, 10, 100)\nshaped = rewards ** 0.5\nprint(\"Shaped Rewards:\", shaped[:5])\n\nConclusion: Adaptive reward shaping speeds up training.\n\n\n\n\n\nMultimodal PCL detection using video and text features.\n\ntext_features = [0.8, 0.9, 0.85]\nvideo_features = [0.7, 0.75, 0.78]\ncombined = [ (t+v)/2 for t, v in zip(text_features, video_features)]\nprint(\"Combined Features:\", combined)"
  },
  {
    "objectID": "paper4.html#research-updates-1",
    "href": "paper4.html#research-updates-1",
    "title": "Study Notes",
    "section": "Research Updates",
    "text": "Research Updates\n\nReinforcement LearningMultimodal PCL\n\n\n\nPaper 1 - State Action Policy (Noise Simulation)\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.plot(np.random.randn(100).cumsum())\nplt.title(\"RL Noise Simulation\")\nplt.show()\n\nConclusion: RL model adapts well under noisy settings.\n\n\nPaper 2 - State Action Policy (Reward Shaping)\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nrewards = np.linspace(0, 10, 100)\nshaped = rewards ** 0.5\nprint(\"Shaped Rewards:\", shaped[:5])\n\nConclusion: Adaptive reward shaping speeds up training.\n\n\n\n\nPaper 3 - Multimodal PCL Detection\nMultimodal PCL detection using video and text features.\n\ntext_features = [0.8, 0.9, 0.85]\nvideo_features = [0.7, 0.75, 0.78]\ncombined = [ (t+v)/2 for t, v in zip(text_features, video_features)]\nprint(\"Combined Features:\", combined)\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.plot(np.random.randn(100).cumsum())\nplt.title(\"RL Noise Simulation\")\nplt.show()"
  },
  {
    "objectID": "paper2.html",
    "href": "paper2.html",
    "title": "Note 2 - Bellman optimality equation (Section 3)",
    "section": "",
    "text": "⬅ Back\n\nSection 3\n复习之前的工作 State value我认为是一种全局的价值，而action value决定了我们的具体策略选择。\n贝尔曼公式就是为了计算state value。\n我们先通过贝尔曼矩阵式计算出state value,再利用定义式拆分出对应的action value\n\nBellman Optimal Equation\n\nImage source: Shiyu Zhao, Westlake University\n\n重要区别\n贝尔曼公式计算的是 给定策略的state value ko 最优贝尔曼计算的是 最优策略下的state value\n普通贝尔曼公式给出的 state value 反映了在某策略下从某状态出发的预期回报，但它并不保证是最优策略。\n最优贝尔曼公式的意义在于，通过递归计算最优的 state value 或 action value，帮助找到能最大化回报的策略。它通过选择最大回报的动作，最优贝尔曼的action value是推算出来的，且是唯一最优的！如下图，左边是经BOE得到的最优action,右边是action对应的全局state value。这是一个自动优化得到的过程。\n\nImage source: Shiyu Zhao, Westlake University"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hongbo Wang",
    "section": "",
    "text": "M.Eng. at Dalian University of Technology (DUTIR Lab)\nIncoming Ph.D. Student at University of Tokyo (Echizen Lab)\nResearching Multimodal Toxicity Detection, LLM, and Responsible AI"
  },
  {
    "objectID": "index.html#hi-i-am-hongbo-wang",
    "href": "index.html#hi-i-am-hongbo-wang",
    "title": "Hongbo Wang",
    "section": "",
    "text": "M.Eng. at Dalian University of Technology (DUTIR Lab)\nIncoming Ph.D. Student at University of Tokyo (Echizen Lab)\nResearching Multimodal Toxicity Detection, LLM, and Responsible AI"
  },
  {
    "objectID": "index.html#research-motivation",
    "href": "index.html#research-motivation",
    "title": "Hongbo Wang",
    "section": "Research Motivation",
    "text": "Research Motivation\nSince 2022, I have been dedicated to researching various forms of toxic speech, including hate speech, offensive language, and patronizing and condescending language (PCL). This line of research includes the construction of a PCL dataset (NLPCC’23), an LLM-based PCL detector (EMNLP’24), and the multimodal PCL detection (ICASSP’25).\nMy research is driven by the goal of protecting vulnerable groups from toxic online discourse and investigating novel cybersecurity paradigms in the context of LLMs/MLLMs."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Hongbo Wang",
    "section": "Education",
    "text": "Education\n\nPh.D. in Information and Communication Engineering, University of Tokyo, 2025 -\n\nM.Eng. in Computer Science and Technology, Dalian University of Technology, 2022 - 2025\n\nB.Eng. in Computer Science and Technology, Dalian University of Technology, 2017 - 2022"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Hongbo Wang",
    "section": "Publications",
    "text": "Publications\n\nCCPC: A Hierarchical Chinese Corpus for Patronizing and Condescending Language Detection\nNLPCC 2023\nHongbo Wang, Mingda Li, Junyu Lu, Liang Yang, Hebin Xia, Hongfei Lin\n[PDF] [Code]\nPclGPT: A Large Language Model for Patronizing and Condescending Language Detection\nEMNLP 2024 (Findings)\nHongbo Wang, Mingda Li, Junyu Lu, Hebin Xia, Liang Yang, Bo Xu, Ruizhu Liu, Hongfei Lin\n[PDF] [Code]\nTowards Patronizing and Condescending Language in Chinese Videos: A Multimodal Dataset and Detector\nICASSP 2025\nHongbo Wang, Junyu Lu, Yan Han, Kai Ma, Liang Yang, Hongfei Lin\n[PDF] [Code]\nGuts at SemEval-2022 Task 4: Adversarial Training and Balancing Methods for Patronizing and Condescending Language Detection\nSemEval-2022\nJunyu Lu, Hao Zhang, Tongyue Zhang, Hongbo Wang, Haohao Zhu, Bo Xu, Hongfei Lin\n[PDF]\nTowards Comprehensive Detection of Chinese Harmful Memes\nNeurIPS 2024\nJunyu Lu, Bo Xu, Xiaokun Zhang, Hongbo Wang, Haohao Zhu, Dongyu Zhang, Liang Yang, Hongfei Lin\n[PDF] [Code]"
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "Hongbo Wang",
    "section": "Skills",
    "text": "Skills\n\n\nPython\n\n\nDeep Learning\n\n\nLinguistic Analysis"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Hongbo Wang",
    "section": "Contact",
    "text": "Contact\n📧 dutlaowang@mail.dlut.edu.cn\n Google Scholar |  GitHub |  Xiaohongshu"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Study Notes",
    "section": "",
    "text": "Note\n\n\n\n\n\n📝 Note 1 - Reinforcement learning (Section 1 & 2)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n📝 Note 2 - Bellman optimality equation (Section 3)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n📝 Note 3 - Value Iteration && Policy Iteration(Section 4)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n📝 Note 4 - Building\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n📝 Note 5 - Building\n\n\n\n\n\n\n\nCategories\n\n📚 All (5)\n\n📝 RL (5)\n\n🔧 Building (2)"
  },
  {
    "objectID": "about.html#research-updates",
    "href": "about.html#research-updates",
    "title": "Study Notes",
    "section": "",
    "text": "Note\n\n\n\n\n\n📝 Note 1 - Reinforcement learning (Section 1 & 2)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n📝 Note 2 - Bellman optimality equation (Section 3)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n📝 Note 3 - Value Iteration && Policy Iteration(Section 4)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n📝 Note 4 - Building\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n📝 Note 5 - Building\n\n\n\n\n\n\n\nCategories\n\n📚 All (5)\n\n📝 RL (5)\n\n🔧 Building (2)"
  },
  {
    "objectID": "paper1.html",
    "href": "paper1.html",
    "title": "Note 1 - Reinforcement learning (Section 1 & 2)",
    "section": "",
    "text": "⬅ Back\n\nSection 1.1 - Basic concepts\nIllustrative example: Grid-world\n\nAccessible / forbidden / target cells\n\nState\nThe status of the agent with respect to the environment.\nFor Grid-world, state refers to the position.\nAction\nAn action leads to a change in state, for example:\n\\[\ns_1 \\xrightarrow{a_2} s_2\n\\]\nForbidden area\nEntering it will result in a penalty.\nPolicy\nProvides a recommendation for which action to take given a state, e.g., choosing a₁ or a₂.\nFor example:\n\\[\np(s_2 \\mid s_1, a_2) = 1\n\\]\nThis means that the policy for transitioning from s₁ to s₂ must use action a₂.\nReward\nA real number providing feedback on the choices already made.\n\nGreater than 0 (&gt; 0) indicates encouragement\n\nLess than 0 (&lt; 0) indicates punishment\nIt helps the agent reduce the tendency to enter forbidden areas.\n\nTrajectory\nA sequence of state-action-reward transitions.\nThe return is the sum of the rewards collected along the trajectory. By comparing the return values of different trajectories, we can determine which overall policy performs better.\nDiscounted return\nBy applying the discount factor γ, we can place more emphasis on either short-term or long-term rewards.\nEpisode\nWhen the number of states is finite, the resulting trajectory is referred to as an episode.\nMarkov Decision Process\nMost importantly, the next decision is not influenced by previous decisions, which is known as the property of being memoryless.\n\n\nSection 1.2 - Connection\nThese concepts are interconnected. The transition from one state to another, which generates a reward, is illustrated in Figure 1.\n\nImage source: Shiyu Zhao, Westlake University\nThe accumulation of multiple states, forming a Trajectory (or Episode), and eventually leading to a return is shown in Figure 2.\n\nImage source: Shiyu Zhao, Westlake University\n\n\nSection 2 - Bellman Equation\nBootstrapping\nThe value of a state depends on the estimated values of subsequent states.\n\nReturn\nThe cumulative sum of rewards collected over time, representing the long-term value of a trajectory.\n\nIt is the total discounted reward from a given state, considering both immediate and future rewards.\nMathematically, it is defined as:\n\\[\nG_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots\n\\]\n\\[\nG_t = R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + \\dots)\n\\]\n\\[\nG_t = R_{t+1} + \\gamma G_{t+1}\n\\]\n\n\nState Value && Bellman Equation\nIn reinforcement learning, the state value refers to the expected return starting from a given state, following a particular policy.\n\nIt quantifies how good it is for the agent to be in a state ( s ).\nFormally defined as:\n\\[\nv_\\pi(s) = \\mathbb{E}_{\\pi}\\left[G_t \\mid s_t = s\\right]\n\\]\nwhich can be expressed as:\n\\[\nv_\\pi(s) = \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} \\mid S_t = s]\n\\]\n\\[  \nv_\\pi(s) = \\sum_{a} \\pi(a|s) \\left[ \\sum_{r} p(r|s,a) r + \\gamma \\sum_{s'} p(s'|s,a) v_\\pi(s') \\right]\n\\]\n\n\nMatrix Form of Bellman Equation\nThe Bellman equation in matrix form can be written as:\n\\[\n\\mathbf{v_\\pi} = \\mathbf{R_\\pi} + \\gamma \\mathbf{P_\\pi} \\mathbf{v_\\pi}\n\\] \nImage source: Shiyu Zhao, Westlake University\n\nAction Value Function\nState value represents the expected long-term return an agent will receive starting from a state, while following a specific policy.\nAction value represents the expected long-term return an agent will receive after taking a specific action in a state, and then continuing to follow a specific policy."
  },
  {
    "objectID": "paper3.html",
    "href": "paper3.html",
    "title": "Study Notes",
    "section": "",
    "text": "Reinforcement LearningMultimodal PCL\n\n\n\n\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.plot(np.random.randn(100).cumsum())\nplt.title(\"RL Noise Simulation\")\nplt.show()\n\nConclusion: RL model adapts well under noisy settings.\n\n\n\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nrewards = np.linspace(0, 10, 100)\nshaped = rewards ** 0.5\nprint(\"Shaped Rewards:\", shaped[:5])\n\nConclusion: Adaptive reward shaping speeds up training.\n\n\n\n\n\nMultimodal PCL detection using video and text features.\n\ntext_features = [0.8, 0.9, 0.85]\nvideo_features = [0.7, 0.75, 0.78]\ncombined = [ (t+v)/2 for t, v in zip(text_features, video_features)]\nprint(\"Combined Features:\", combined)"
  },
  {
    "objectID": "paper3.html#research-updates",
    "href": "paper3.html#research-updates",
    "title": "Study Notes",
    "section": "",
    "text": "Reinforcement LearningMultimodal PCL\n\n\n\n\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.plot(np.random.randn(100).cumsum())\nplt.title(\"RL Noise Simulation\")\nplt.show()\n\nConclusion: RL model adapts well under noisy settings.\n\n\n\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nrewards = np.linspace(0, 10, 100)\nshaped = rewards ** 0.5\nprint(\"Shaped Rewards:\", shaped[:5])\n\nConclusion: Adaptive reward shaping speeds up training.\n\n\n\n\n\nMultimodal PCL detection using video and text features.\n\ntext_features = [0.8, 0.9, 0.85]\nvideo_features = [0.7, 0.75, 0.78]\ncombined = [ (t+v)/2 for t, v in zip(text_features, video_features)]\nprint(\"Combined Features:\", combined)"
  }
]