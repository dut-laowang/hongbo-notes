[
  {
    "objectID": "ppt.html",
    "href": "ppt.html",
    "title": "My PPT Showcase (Click to view PDFs)",
    "section": "",
    "text": "03/15/2025 PPT for ICASSP 2025 Video\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n03/13/2025 Survey on Knowledge Distillation Techniques (LLMs)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n03/02/2025 Survey on DeepSeek V3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n01/21/2025 PhD Application PPT Slides (UTokyo)"
  },
  {
    "objectID": "paper2.html",
    "href": "paper2.html",
    "title": "Study Notes",
    "section": "",
    "text": "Reinforcement LearningMultimodal PCL\n\n\n\n\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.plot(np.random.randn(100).cumsum())\nplt.title(\"RL Noise Simulation\")\nplt.show()\n\nConclusion: RL model adapts well under noisy settings.\n\n\n\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nrewards = np.linspace(0, 10, 100)\nshaped = rewards ** 0.5\nprint(\"Shaped Rewards:\", shaped[:5])\n\nConclusion: Adaptive reward shaping speeds up training.\n\n\n\n\n\nMultimodal PCL detection using video and text features.\n\ntext_features = [0.8, 0.9, 0.85]\nvideo_features = [0.7, 0.75, 0.78]\ncombined = [ (t+v)/2 for t, v in zip(text_features, video_features)]\nprint(\"Combined Features:\", combined)"
  },
  {
    "objectID": "paper2.html#research-updates",
    "href": "paper2.html#research-updates",
    "title": "Study Notes",
    "section": "",
    "text": "Reinforcement LearningMultimodal PCL\n\n\n\n\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.plot(np.random.randn(100).cumsum())\nplt.title(\"RL Noise Simulation\")\nplt.show()\n\nConclusion: RL model adapts well under noisy settings.\n\n\n\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nrewards = np.linspace(0, 10, 100)\nshaped = rewards ** 0.5\nprint(\"Shaped Rewards:\", shaped[:5])\n\nConclusion: Adaptive reward shaping speeds up training.\n\n\n\n\n\nMultimodal PCL detection using video and text features.\n\ntext_features = [0.8, 0.9, 0.85]\nvideo_features = [0.7, 0.75, 0.78]\ncombined = [ (t+v)/2 for t, v in zip(text_features, video_features)]\nprint(\"Combined Features:\", combined)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hongbo Wang",
    "section": "",
    "text": "M.Eng. at Dalian University of Technology (DUTIR Lab)\nIncoming Ph.D.¬†Student at University of Tokyo (Echizen Lab)\nResearching Multimodal Toxicity Detection, LLM, and Responsible AI"
  },
  {
    "objectID": "index.html#hi-i-am-hongbo-wang",
    "href": "index.html#hi-i-am-hongbo-wang",
    "title": "Hongbo Wang",
    "section": "",
    "text": "M.Eng. at Dalian University of Technology (DUTIR Lab)\nIncoming Ph.D.¬†Student at University of Tokyo (Echizen Lab)\nResearching Multimodal Toxicity Detection, LLM, and Responsible AI"
  },
  {
    "objectID": "index.html#research-motivation",
    "href": "index.html#research-motivation",
    "title": "Hongbo Wang",
    "section": "Research Motivation",
    "text": "Research Motivation\nSince 2022, I have been dedicated to researching various forms of toxic speech, including hate speech, offensive language, and patronizing and condescending language (PCL). This line of research includes the construction of a PCL dataset (NLPCC‚Äô23), an LLM-based PCL detector (EMNLP‚Äô24), and the multimodal PCL detection (ICASSP‚Äô25).\nMy research is driven by the goal of protecting vulnerable groups from toxic online discourse and investigating novel cybersecurity paradigms in the context of LLMs/MLLMs."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Hongbo Wang",
    "section": "Education",
    "text": "Education\n\nPh.D.¬†in Information and Communication Engineering, University of Tokyo, 2025 -\n\nM.Eng. in Computer Science and Technology, Dalian University of Technology, 2022 - 2025\n\nB.Eng. in Computer Science and Technology, Dalian University of Technology, 2017 - 2022"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Hongbo Wang",
    "section": "Publications",
    "text": "Publications\n\nCCPC: A Hierarchical Chinese Corpus for Patronizing and Condescending Language Detection\nNLPCC 2023\nHongbo Wang, Mingda Li, Junyu Lu, Liang Yang, Hebin Xia, Hongfei Lin\n[PDF] [Code]\nPclGPT: A Large Language Model for Patronizing and Condescending Language Detection\nEMNLP 2024 (Findings)\nHongbo Wang, Mingda Li, Junyu Lu, Hebin Xia, Liang Yang, Bo Xu, Ruizhu Liu, Hongfei Lin\n[PDF] [Code]\nTowards Patronizing and Condescending Language in Chinese Videos: A Multimodal Dataset and Detector\nICASSP 2025\nHongbo Wang, Junyu Lu, Yan Han, Kai Ma, Liang Yang, Hongfei Lin\n[PDF] [Code]\nGuts at SemEval-2022 Task 4: Adversarial Training and Balancing Methods for Patronizing and Condescending Language Detection\nSemEval-2022\nJunyu Lu, Hao Zhang, Tongyue Zhang, Hongbo Wang, Haohao Zhu, Bo Xu, Hongfei Lin\n[PDF]\nTowards Comprehensive Detection of Chinese Harmful Memes\nNeurIPS 2024\nJunyu Lu, Bo Xu, Xiaokun Zhang, Hongbo Wang, Haohao Zhu, Dongyu Zhang, Liang Yang, Hongfei Lin\n[PDF] [Code]"
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "Hongbo Wang",
    "section": "Skills",
    "text": "Skills\n\n\nPython\n\n\nDeep Learning\n\n\nLinguistic Analysis"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Hongbo Wang",
    "section": "Contact",
    "text": "Contact\nüìß dutlaowang@mail.dlut.edu.cn\n Google Scholar |  GitHub |  Xiaohongshu"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Study Notes",
    "section": "",
    "text": "Note\n\n\n\n\n\nüìù Note 1 - Reinforcement learning (Section 1 & 2)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nüìù Note 2 - Building\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nüìù Note 3 - Building\n\n\n\n\n\n\n\nCategories\n\nüìö All (3)\n\nüìù RL (1)\n\nüîß Building (2)"
  },
  {
    "objectID": "about.html#research-updates",
    "href": "about.html#research-updates",
    "title": "Study Notes",
    "section": "",
    "text": "Note\n\n\n\n\n\nüìù Note 1 - Reinforcement learning (Section 1 & 2)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nüìù Note 2 - Building\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nüìù Note 3 - Building\n\n\n\n\n\n\n\nCategories\n\nüìö All (3)\n\nüìù RL (1)\n\nüîß Building (2)"
  },
  {
    "objectID": "paper1.html",
    "href": "paper1.html",
    "title": "Note 1 - Reinforcement learning (Section 1 & 2)",
    "section": "",
    "text": "‚¨Ö Back\n\nSection 1.1 - Basic concepts\nIllustrative example: Grid-world\n\nAccessible / forbidden / target cells\n\nState\nThe status of the agent with respect to the environment.\nFor Grid-world, state refers to the position.\nAction\nAn action leads to a change in state, for example:\n\\[\ns_1 \\xrightarrow{a_2} s_2\n\\]\nForbidden area\nEntering it will result in a penalty.\nPolicy\nProvides a recommendation for which action to take given a state, e.g., choosing a‚ÇÅ or a‚ÇÇ.\nFor example:\n\\[\np(s_2 \\mid s_1, a_2) = 1\n\\]\nThis means that the policy for transitioning from s‚ÇÅ to s‚ÇÇ must use action a‚ÇÇ.\nReward\nA real number providing feedback on the choices already made.\n\nGreater than 0 (&gt; 0) indicates encouragement\n\nLess than 0 (&lt; 0) indicates punishment\nIt helps the agent reduce the tendency to enter forbidden areas.\n\nTrajectory\nA sequence of state-action-reward transitions.\nThe return is the sum of the rewards collected along the trajectory. By comparing the return values of different trajectories, we can determine which overall policy performs better.\nDiscounted return\nBy applying the discount factor Œ≥, we can place more emphasis on either short-term or long-term rewards.\nEpisode\nWhen the number of states is finite, the resulting trajectory is referred to as an episode.\nMarkov Decision Process\nMost importantly, the next decision is not influenced by previous decisions, which is known as the property of being memoryless.\n\n\nSection 1.2 - Connection\nThese concepts are interconnected. The transition from one state to another, which generates a reward, is illustrated in Figure 1.\n\nImage source: Shiyu Zhao, Westlake University\nThe accumulation of multiple states, forming a Trajectory (or Episode), and eventually leading to a return is shown in Figure 2.\n\nImage source: Shiyu Zhao, Westlake University\n\n\nSection 2 - Bellman Equation\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.plot(np.random.randn(100).cumsum())\nplt.title(\"RL Noise Simulation\")\nplt.show()\n\n\n\n\n\n\n\n\n\nConclusion: RL model adapts well under noisy settings."
  },
  {
    "objectID": "paper3.html",
    "href": "paper3.html",
    "title": "Study Notes",
    "section": "",
    "text": "Reinforcement LearningMultimodal PCL\n\n\n\n\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.plot(np.random.randn(100).cumsum())\nplt.title(\"RL Noise Simulation\")\nplt.show()\n\nConclusion: RL model adapts well under noisy settings.\n\n\n\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nrewards = np.linspace(0, 10, 100)\nshaped = rewards ** 0.5\nprint(\"Shaped Rewards:\", shaped[:5])\n\nConclusion: Adaptive reward shaping speeds up training.\n\n\n\n\n\nMultimodal PCL detection using video and text features.\n\ntext_features = [0.8, 0.9, 0.85]\nvideo_features = [0.7, 0.75, 0.78]\ncombined = [ (t+v)/2 for t, v in zip(text_features, video_features)]\nprint(\"Combined Features:\", combined)"
  },
  {
    "objectID": "paper3.html#research-updates",
    "href": "paper3.html#research-updates",
    "title": "Study Notes",
    "section": "",
    "text": "Reinforcement LearningMultimodal PCL\n\n\n\n\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.plot(np.random.randn(100).cumsum())\nplt.title(\"RL Noise Simulation\")\nplt.show()\n\nConclusion: RL model adapts well under noisy settings.\n\n\n\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nrewards = np.linspace(0, 10, 100)\nshaped = rewards ** 0.5\nprint(\"Shaped Rewards:\", shaped[:5])\n\nConclusion: Adaptive reward shaping speeds up training.\n\n\n\n\n\nMultimodal PCL detection using video and text features.\n\ntext_features = [0.8, 0.9, 0.85]\nvideo_features = [0.7, 0.75, 0.78]\ncombined = [ (t+v)/2 for t, v in zip(text_features, video_features)]\nprint(\"Combined Features:\", combined)"
  }
]