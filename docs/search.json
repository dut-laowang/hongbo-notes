[
  {
    "objectID": "ppt.html",
    "href": "ppt.html",
    "title": "My PPT Showcase (Click to view PDFs)",
    "section": "",
    "text": "03/15/2025 PPT for ICASSP 2025 Video\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n03/13/2025 Survey on Knowledge Distillation Techniques (LLMs)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n03/02/2025 Survey on DeepSeek V3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n01/21/2025 PhD Application PPT Slides (UTokyo)"
  },
  {
    "objectID": "paper4.html",
    "href": "paper4.html",
    "title": "Study Notes",
    "section": "",
    "text": "Reinforcement LearningMultimodal PCL\n\n\n\n\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.plot(np.random.randn(100).cumsum())\nplt.title(\"RL Noise Simulation\")\nplt.show()\n\nConclusion: RL model adapts well under noisy settings.\n\n\n\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nrewards = np.linspace(0, 10, 100)\nshaped = rewards ** 0.5\nprint(\"Shaped Rewards:\", shaped[:5])\n\nConclusion: Adaptive reward shaping speeds up training.\n\n\n\n\n\nMultimodal PCL detection using video and text features.\n\ntext_features = [0.8, 0.9, 0.85]\nvideo_features = [0.7, 0.75, 0.78]\ncombined = [ (t+v)/2 for t, v in zip(text_features, video_features)]\nprint(\"Combined Features:\", combined)"
  },
  {
    "objectID": "paper4.html#research-updates",
    "href": "paper4.html#research-updates",
    "title": "Study Notes",
    "section": "",
    "text": "Reinforcement LearningMultimodal PCL\n\n\n\n\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.plot(np.random.randn(100).cumsum())\nplt.title(\"RL Noise Simulation\")\nplt.show()\n\nConclusion: RL model adapts well under noisy settings.\n\n\n\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nrewards = np.linspace(0, 10, 100)\nshaped = rewards ** 0.5\nprint(\"Shaped Rewards:\", shaped[:5])\n\nConclusion: Adaptive reward shaping speeds up training.\n\n\n\n\n\nMultimodal PCL detection using video and text features.\n\ntext_features = [0.8, 0.9, 0.85]\nvideo_features = [0.7, 0.75, 0.78]\ncombined = [ (t+v)/2 for t, v in zip(text_features, video_features)]\nprint(\"Combined Features:\", combined)"
  },
  {
    "objectID": "paper4.html#research-updates-1",
    "href": "paper4.html#research-updates-1",
    "title": "Study Notes",
    "section": "Research Updates",
    "text": "Research Updates\n\nReinforcement LearningMultimodal PCL\n\n\n\nPaper 1 - State Action Policy (Noise Simulation)\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.plot(np.random.randn(100).cumsum())\nplt.title(\"RL Noise Simulation\")\nplt.show()\n\nConclusion: RL model adapts well under noisy settings.\n\n\nPaper 2 - State Action Policy (Reward Shaping)\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nrewards = np.linspace(0, 10, 100)\nshaped = rewards ** 0.5\nprint(\"Shaped Rewards:\", shaped[:5])\n\nConclusion: Adaptive reward shaping speeds up training.\n\n\n\n\nPaper 3 - Multimodal PCL Detection\nMultimodal PCL detection using video and text features.\n\ntext_features = [0.8, 0.9, 0.85]\nvideo_features = [0.7, 0.75, 0.78]\ncombined = [ (t+v)/2 for t, v in zip(text_features, video_features)]\nprint(\"Combined Features:\", combined)\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.plot(np.random.randn(100).cumsum())\nplt.title(\"RL Noise Simulation\")\nplt.show()"
  },
  {
    "objectID": "paper2.html",
    "href": "paper2.html",
    "title": "Note 2 - Bellman optimality equation (Section 3)",
    "section": "",
    "text": "â¬… Back\n\nSection 3\nå¤ä¹ ä¹‹å‰çš„å·¥ä½œ State valueæˆ‘è®¤ä¸ºæ˜¯ä¸€ç§å…¨å±€çš„ä»·å€¼ï¼Œè€Œaction valueå†³å®šäº†æˆ‘ä»¬çš„å…·ä½“ç­–ç•¥é€‰æ‹©ã€‚\nè´å°”æ›¼å…¬å¼å°±æ˜¯ä¸ºäº†è®¡ç®—state valueã€‚\næˆ‘ä»¬å…ˆé€šè¿‡è´å°”æ›¼çŸ©é˜µå¼è®¡ç®—å‡ºstate value,å†åˆ©ç”¨å®šä¹‰å¼æ‹†åˆ†å‡ºå¯¹åº”çš„action value\n\nBellman Optimal Equation\n\nImage source: Shiyu Zhao, Westlake University\n\né‡è¦åŒºåˆ«\nè´å°”æ›¼å…¬å¼è®¡ç®—çš„æ˜¯ ç»™å®šç­–ç•¥çš„state value ko æœ€ä¼˜è´å°”æ›¼è®¡ç®—çš„æ˜¯ æœ€ä¼˜ç­–ç•¥ä¸‹çš„state value\næ™®é€šè´å°”æ›¼å…¬å¼ç»™å‡ºçš„ state value åæ˜ äº†åœ¨æŸç­–ç•¥ä¸‹ä»æŸçŠ¶æ€å‡ºå‘çš„é¢„æœŸå›æŠ¥ï¼Œä½†å®ƒå¹¶ä¸ä¿è¯æ˜¯æœ€ä¼˜ç­–ç•¥ã€‚\næœ€ä¼˜è´å°”æ›¼å…¬å¼çš„æ„ä¹‰åœ¨äºï¼Œé€šè¿‡é€’å½’è®¡ç®—æœ€ä¼˜çš„ state value æˆ– action valueï¼Œå¸®åŠ©æ‰¾åˆ°èƒ½æœ€å¤§åŒ–å›æŠ¥çš„ç­–ç•¥ã€‚å®ƒé€šè¿‡é€‰æ‹©æœ€å¤§å›æŠ¥çš„åŠ¨ä½œï¼Œæœ€ä¼˜è´å°”æ›¼çš„action valueæ˜¯æ¨ç®—å‡ºæ¥çš„ï¼Œä¸”æ˜¯å”¯ä¸€æœ€ä¼˜çš„ï¼å¦‚ä¸‹å›¾ï¼Œå·¦è¾¹æ˜¯ç»BOEå¾—åˆ°çš„æœ€ä¼˜action,å³è¾¹æ˜¯actionå¯¹åº”çš„å…¨å±€state valueã€‚è¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨ä¼˜åŒ–å¾—åˆ°çš„è¿‡ç¨‹ã€‚\n\nImage source: Shiyu Zhao, Westlake University"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hongbo Wang",
    "section": "",
    "text": "M.Eng. at Dalian University of Technology (DUTIR Lab)\nIncoming Ph.D.Â Student at University of Tokyo (Echizen Lab)\nResearching Multimodal Toxicity Detection, LLM, and Responsible AI"
  },
  {
    "objectID": "index.html#hi-i-am-hongbo-wang",
    "href": "index.html#hi-i-am-hongbo-wang",
    "title": "Hongbo Wang",
    "section": "",
    "text": "M.Eng. at Dalian University of Technology (DUTIR Lab)\nIncoming Ph.D.Â Student at University of Tokyo (Echizen Lab)\nResearching Multimodal Toxicity Detection, LLM, and Responsible AI"
  },
  {
    "objectID": "index.html#research-motivation",
    "href": "index.html#research-motivation",
    "title": "Hongbo Wang",
    "section": "Research Motivation",
    "text": "Research Motivation\nSince 2022, I have been dedicated to researching various forms of toxic speech, including hate speech, offensive language, and patronizing and condescending language (PCL). This line of research includes the construction of a PCL dataset (NLPCCâ€™23), an LLM-based PCL detector (EMNLPâ€™24), and the multimodal PCL detection (ICASSPâ€™25).\nMy research is driven by the goal of protecting vulnerable groups from toxic online discourse and investigating novel cybersecurity paradigms in the context of LLMs/MLLMs."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Hongbo Wang",
    "section": "Education",
    "text": "Education\n\nPh.D.Â in Information and Communication Engineering, University of Tokyo, 2025 -\n\nM.Eng. in Computer Science and Technology, Dalian University of Technology, 2022 - 2025\n\nB.Eng. in Computer Science and Technology, Dalian University of Technology, 2017 - 2022"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Hongbo Wang",
    "section": "Publications",
    "text": "Publications\n\nCCPC: A Hierarchical Chinese Corpus for Patronizing and Condescending Language Detection\nNLPCC 2023\nHongbo Wang, Mingda Li, Junyu Lu, Liang Yang, Hebin Xia, Hongfei Lin\n[PDF] [Code]\nPclGPT: A Large Language Model for Patronizing and Condescending Language Detection\nEMNLP 2024 (Findings)\nHongbo Wang, Mingda Li, Junyu Lu, Hebin Xia, Liang Yang, Bo Xu, Ruizhu Liu, Hongfei Lin\n[PDF] [Code]\nTowards Patronizing and Condescending Language in Chinese Videos: A Multimodal Dataset and Detector\nICASSP 2025\nHongbo Wang, Junyu Lu, Yan Han, Kai Ma, Liang Yang, Hongfei Lin\n[PDF] [Code]\nGuts at SemEval-2022 Task 4: Adversarial Training and Balancing Methods for Patronizing and Condescending Language Detection\nSemEval-2022\nJunyu Lu, Hao Zhang, Tongyue Zhang, Hongbo Wang, Haohao Zhu, Bo Xu, Hongfei Lin\n[PDF]\nTowards Comprehensive Detection of Chinese Harmful Memes\nNeurIPS 2024\nJunyu Lu, Bo Xu, Xiaokun Zhang, Hongbo Wang, Haohao Zhu, Dongyu Zhang, Liang Yang, Hongfei Lin\n[PDF] [Code]"
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "Hongbo Wang",
    "section": "Skills",
    "text": "Skills\n\n\nPython\n\n\nDeep Learning\n\n\nLinguistic Analysis"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Hongbo Wang",
    "section": "Contact",
    "text": "Contact\nğŸ“§ dutlaowang@mail.dlut.edu.cn\n Google Scholar |  GitHub |  Xiaohongshu"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Study Notes",
    "section": "",
    "text": "Note\n\n\n\n\n\nğŸ“ Note 1 - Reinforcement learning (Section 1 & 2)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nğŸ“ Note 2 - Bellman optimality equation (Section 3)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nğŸ“ Note 3 - Value Iteration && Policy Iteration(Section 4)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nğŸ“ Note 4 - Building\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nğŸ“ Note 5 - Building\n\n\n\n\n\n\n\nCategories\n\nğŸ“š All (5)\n\nğŸ“ RL (5)\n\nğŸ”§ Building (2)"
  },
  {
    "objectID": "about.html#research-updates",
    "href": "about.html#research-updates",
    "title": "Study Notes",
    "section": "",
    "text": "Note\n\n\n\n\n\nğŸ“ Note 1 - Reinforcement learning (Section 1 & 2)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nğŸ“ Note 2 - Bellman optimality equation (Section 3)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nğŸ“ Note 3 - Value Iteration && Policy Iteration(Section 4)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nğŸ“ Note 4 - Building\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nğŸ“ Note 5 - Building\n\n\n\n\n\n\n\nCategories\n\nğŸ“š All (5)\n\nğŸ“ RL (5)\n\nğŸ”§ Building (2)"
  },
  {
    "objectID": "paper1.html",
    "href": "paper1.html",
    "title": "Note 1 - Reinforcement learning (Section 1 & 2)",
    "section": "",
    "text": "â¬… Back\n\nSection 1.1 - Basic concepts\nIllustrative example: Grid-world\n\nAccessible / forbidden / target cells\n\nState\nThe status of the agent with respect to the environment.\nFor Grid-world, state refers to the position.\nAction\nAn action leads to a change in state, for example:\n\\[\ns_1 \\xrightarrow{a_2} s_2\n\\]\nForbidden area\nEntering it will result in a penalty.\nPolicy\nProvides a recommendation for which action to take given a state, e.g., choosing aâ‚ or aâ‚‚.\nFor example:\n\\[\np(s_2 \\mid s_1, a_2) = 1\n\\]\nThis means that the policy for transitioning from sâ‚ to sâ‚‚ must use action aâ‚‚.\nReward\nA real number providing feedback on the choices already made.\n\nGreater than 0 (&gt; 0) indicates encouragement\n\nLess than 0 (&lt; 0) indicates punishment\nIt helps the agent reduce the tendency to enter forbidden areas.\n\nTrajectory\nA sequence of state-action-reward transitions.\nThe return is the sum of the rewards collected along the trajectory. By comparing the return values of different trajectories, we can determine which overall policy performs better.\nDiscounted return\nBy applying the discount factor Î³, we can place more emphasis on either short-term or long-term rewards.\nEpisode\nWhen the number of states is finite, the resulting trajectory is referred to as an episode.\nMarkov Decision Process\nMost importantly, the next decision is not influenced by previous decisions, which is known as the property of being memoryless.\n\n\nSection 1.2 - Connection\nThese concepts are interconnected. The transition from one state to another, which generates a reward, is illustrated in Figure 1.\n\nImage source: Shiyu Zhao, Westlake University\nThe accumulation of multiple states, forming a Trajectory (or Episode), and eventually leading to a return is shown in Figure 2.\n\nImage source: Shiyu Zhao, Westlake University\n\n\nSection 2 - Bellman Equation\nBootstrapping\nThe value of a state depends on the estimated values of subsequent states.\n\nReturn\nThe cumulative sum of rewards collected over time, representing the long-term value of a trajectory.\n\nIt is the total discounted reward from a given state, considering both immediate and future rewards.\nMathematically, it is defined as:\n\\[\nG_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots\n\\]\n\\[\nG_t = R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + \\dots)\n\\]\n\\[\nG_t = R_{t+1} + \\gamma G_{t+1}\n\\]\n\n\nState Value && Bellman Equation\nIn reinforcement learning, the state value refers to the expected return starting from a given state, following a particular policy.\n\nIt quantifies how good it is for the agent to be in a state ( s ).\nFormally defined as:\n\\[\nv_\\pi(s) = \\mathbb{E}_{\\pi}\\left[G_t \\mid s_t = s\\right]\n\\]\nwhich can be expressed as:\n\\[\nv_\\pi(s) = \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} \\mid S_t = s]\n\\]\n\\[  \nv_\\pi(s) = \\sum_{a} \\pi(a|s) \\left[ \\sum_{r} p(r|s,a) r + \\gamma \\sum_{s'} p(s'|s,a) v_\\pi(s') \\right]\n\\]\n\n\nMatrix Form of Bellman Equation\nThe Bellman equation in matrix form can be written as:\n\\[\n\\mathbf{v_\\pi} = \\mathbf{R_\\pi} + \\gamma \\mathbf{P_\\pi} \\mathbf{v_\\pi}\n\\] \nImage source: Shiyu Zhao, Westlake University\n\nAction Value Function\nState value represents the expected long-term return an agent will receive starting from a state, while following a specific policy.\nAction value represents the expected long-term return an agent will receive after taking a specific action in a state, and then continuing to follow a specific policy."
  },
  {
    "objectID": "paper3.html",
    "href": "paper3.html",
    "title": "Study Notes",
    "section": "",
    "text": "Reinforcement LearningMultimodal PCL\n\n\n\n\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.plot(np.random.randn(100).cumsum())\nplt.title(\"RL Noise Simulation\")\nplt.show()\n\nConclusion: RL model adapts well under noisy settings.\n\n\n\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nrewards = np.linspace(0, 10, 100)\nshaped = rewards ** 0.5\nprint(\"Shaped Rewards:\", shaped[:5])\n\nConclusion: Adaptive reward shaping speeds up training.\n\n\n\n\n\nMultimodal PCL detection using video and text features.\n\ntext_features = [0.8, 0.9, 0.85]\nvideo_features = [0.7, 0.75, 0.78]\ncombined = [ (t+v)/2 for t, v in zip(text_features, video_features)]\nprint(\"Combined Features:\", combined)"
  },
  {
    "objectID": "paper3.html#research-updates",
    "href": "paper3.html#research-updates",
    "title": "Study Notes",
    "section": "",
    "text": "Reinforcement LearningMultimodal PCL\n\n\n\n\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.plot(np.random.randn(100).cumsum())\nplt.title(\"RL Noise Simulation\")\nplt.show()\n\nConclusion: RL model adapts well under noisy settings.\n\n\n\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nrewards = np.linspace(0, 10, 100)\nshaped = rewards ** 0.5\nprint(\"Shaped Rewards:\", shaped[:5])\n\nConclusion: Adaptive reward shaping speeds up training.\n\n\n\n\n\nMultimodal PCL detection using video and text features.\n\ntext_features = [0.8, 0.9, 0.85]\nvideo_features = [0.7, 0.75, 0.78]\ncombined = [ (t+v)/2 for t, v in zip(text_features, video_features)]\nprint(\"Combined Features:\", combined)"
  }
]