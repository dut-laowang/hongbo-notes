[
  {
    "objectID": "study_note.html",
    "href": "study_note.html",
    "title": "Study Notes",
    "section": "",
    "text": "Note\n\n\n\n\n\nüìù Note 1 - Reinforcement learning (Section 1 & 2)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nüìù Note 2 - Bellman optimality equation (Section 3)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nüìù Note 3 - Value Iteration && Policy Iteration(Section 4)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nüìù Note 4 - Building\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nüìù Note 5 - Building\n\n\n\n\n\n\n\nCategories\n\nüìö All (5)\n\nüìù RL (5)\n\nüîß Building (2)"
  },
  {
    "objectID": "paper3.html",
    "href": "paper3.html",
    "title": "Note 3 - Value Iteration && Policy Iteration(Section 4)",
    "section": "",
    "text": "‚¨Ö Back\n‚¨Ö Back\n\nSection 4\nÂ§ç‰π†‰πãÂâçÁöÑÂ∑•‰Ωú In last section, the Bellman Optimal Equation we have learned, is a type of the value iteration."
  },
  {
    "objectID": "paper1.html",
    "href": "paper1.html",
    "title": "Note 1 - Reinforcement learning (Section 1 & 2)",
    "section": "",
    "text": "‚¨Ö Back\n\nSection 1.1 - Basic concepts\nIllustrative example: Grid-world\n\nAccessible / forbidden / target cells\n\nState\nThe status of the agent with respect to the environment.\nFor Grid-world, state refers to the position.\nAction\nAn action leads to a change in state, for example:\n\\[\ns_1 \\xrightarrow{a_2} s_2\n\\]\nForbidden area\nEntering it will result in a penalty.\nPolicy\nProvides a recommendation for which action to take given a state, e.g., choosing a‚ÇÅ or a‚ÇÇ.\nFor example:\n\\[\np(s_2 \\mid s_1, a_2) = 1\n\\]\nThis means that the policy for transitioning from s‚ÇÅ to s‚ÇÇ must use action a‚ÇÇ.\nReward\nA real number providing feedback on the choices already made.\n\nGreater than 0 (&gt; 0) indicates encouragement\n\nLess than 0 (&lt; 0) indicates punishment\nIt helps the agent reduce the tendency to enter forbidden areas.\n\nTrajectory\nA sequence of state-action-reward transitions.\nThe return is the sum of the rewards collected along the trajectory. By comparing the return values of different trajectories, we can determine which overall policy performs better.\nDiscounted return\nBy applying the discount factor Œ≥, we can place more emphasis on either short-term or long-term rewards.\nEpisode\nWhen the number of states is finite, the resulting trajectory is referred to as an episode.\nMarkov Decision Process\nMost importantly, the next decision is not influenced by previous decisions, which is known as the property of being memoryless.\n\n\nSection 1.2 - Connection\nThese concepts are interconnected. The transition from one state to another, which generates a reward, is illustrated in Figure 1.\n\nImage source: Shiyu Zhao, Westlake University\nThe accumulation of multiple states, forming a Trajectory (or Episode), and eventually leading to a return is shown in Figure 2.\n\nImage source: Shiyu Zhao, Westlake University\n\n\nSection 2 - Bellman Equation\nBootstrapping\nThe value of a state depends on the estimated values of subsequent states.\n\nReturn\nThe cumulative sum of rewards collected over time, representing the long-term value of a trajectory.\n\nIt is the total discounted reward from a given state, considering both immediate and future rewards.\nMathematically, it is defined as:\n\\[\nG_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots\n\\]\n\\[\nG_t = R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + \\dots)\n\\]\n\\[\nG_t = R_{t+1} + \\gamma G_{t+1}\n\\]\n\n\nState Value && Bellman Equation\nIn reinforcement learning, the state value refers to the expected return starting from a given state, following a particular policy.\n\nIt quantifies how good it is for the agent to be in a state ( s ).\nFormally defined as:\n\\[\nv_\\pi(s) = \\mathbb{E}_{\\pi}\\left[G_t \\mid s_t = s\\right]\n\\]\nwhich can be expressed as:\n\\[\nv_\\pi(s) = \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} \\mid S_t = s]\n\\]\n\\[  \nv_\\pi(s) = \\sum_{a} \\pi(a|s) \\left[ \\sum_{r} p(r|s,a) r + \\gamma \\sum_{s'} p(s'|s,a) v_\\pi(s') \\right]\n\\]\n\n\nMatrix Form of Bellman Equation\nThe Bellman equation in matrix form can be written as:\n\\[\n\\mathbf{v_\\pi} = \\mathbf{R_\\pi} + \\gamma \\mathbf{P_\\pi} \\mathbf{v_\\pi}\n\\] \nImage source: Shiyu Zhao, Westlake University\n\nAction Value Function\nState value represents the expected long-term return an agent will receive starting from a state, while following a specific policy.\nAction value represents the expected long-term return an agent will receive after taking a specific action in a state, and then continuing to follow a specific policy."
  },
  {
    "objectID": "experiment2.html",
    "href": "experiment2.html",
    "title": "4√ó3 Research Matrix for PHD",
    "section": "",
    "text": "‚¨Ö Back\nüü® ÔºàYellow is the priority taskÔºâ\nüîÑÔºàThis part of the research will be conducted based on the yellow fundamental modulesÔºâ\n\n\n\nProblem / Method\nA. Neuron / Activation\nB. Jailbreak / Red-Team Data\nC. Reasoning Chain + Safety Governance (incl.¬†RL)\n\n\n\n\nP1 Harmful / Jailbreak Suppression\nüü® ARUA-VL. Suppress toxic neurons to reduce harmful generation; includes dynamic activation editing. Refs: Whispering Experts: Neural Interventions for Toxicity Mitigation in Language Models (AurA), arXiv 2024, Finding Safety Neurons in LLMs (OpenReview 2024)\nBaseline: JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal LLMs against Jailbreak Attacks (arXiv 2024). In the fields of text and MLLM, jailbreaks are generated through inductive prompts, and a lot has already been done in this area, making it difficult to achieve breakthroughs.\nHCOT focus: build harmful-vs-safe CoT preference pairs; train with DPO (ICLR 2024) or GRPO (arXiv 2024); compare with RLHF baselines.\n\n\nP2 Multimodal / Video Safety (Temporal Triggers)\nüîÑ Extension of P1-A: apply toxic-neuron suppression methods at the frame/shot level in videos. Detect & suppress dangerous activations using P1-A‚Äôs activation-intervention toolkit; This work should be carried out after the completion of P1-A.\nüü® Planned harmful multimodal video dataset; inspired by Video-SafetyBench (arXiv 2025), SafeVid-350K (arXiv 2025). At the video level, several toxic video datasets have already emerged. I have some ideas about generating harmful videos using large models, and this will be a key direction for the next stage of work.\nVideo CoT refusal + RL: Apply reinforcement learning based on key video frames/features to fine-tune MLLMs and reduce harmful video generation. Use per-frame reasoning chains with trigger timestamps; optimize with Safe-RLHF-V (arXiv 2025).\n\n\nP3 Hallucination / Factual Errors\nEvidence: NAM ‚Äì Neuron Attribution for MLLMs (NeurIPS 2024), enabling analysis of ‚Äúhallucination clusters.‚Äù and searching for neuron models prone to hallucinations.\nThis requires further research.\nCoT √ó RL alignment: Use DPO/GRPO or RLHF variants to prefer evidence-based refusal or factual CoTs over hallucinated outputs. Grounding refs: SelfCheckGPT (ACL 2023), RAHF: Reinforced Alignment for Hallucination Factuality (arXiv 2023).\n\n\nP4 Multilingual / Cultural Contexts\nIt is possible to analyze the perception abilities of different neurons in large models across different languages.\nGap: multilingual jailbreak/red-team datasets (subtitle corruption, cultural memes).\nA future direction is to conduct comparative studies on how chain-of-thought reasoning and reinforcement learning guide content across different cultural contexts, such as Chinese, Japanese, and English.\n\n\n\nNote: The experimental part of AURA-VL has already been completed. I will begin writing the paper in the near future, with the plan to submit it to the target journal."
  },
  {
    "objectID": "experiment1.html",
    "href": "experiment1.html",
    "title": "EX 1 - VERL_PPO",
    "section": "",
    "text": "‚¨Ö Back\n\nReproducing VERL PPO with Pods\nIn this experiment, I learned how to reproduce the PPO algorithm from the VERL framework inside Pods.\nMy main idea is to design a custom reward model to reduce the generation of toxic content.\nThe overall framework is based on DAPO, so I first need to reproduce VERL.\n\n\nUsing Pods\nVERL depends on Docker, but Docker is not supported on Autodl.\nSo I used Pods for remote SSH connection.\nAdvantages:\n1. We can connect to a pre-installed Docker environment:\nwhatcanyousee/verl:ngc-cu124-vllm0.8.3-sglang0.4.5-mcore0.12.0-te2.2\n2. No need for Xshell ‚Äì Pods provide a built-in terminal.\n3. Many available GPUs (like A100) at a reasonable price.\n4. Model download is fast and efficient.\n\n\nInstall VERL\n\nAfter pulling the Docker image, it will create a container under /workspace.\n\nInside the container, install the latest nightly version of VERL:\n\ngit clone https://github.com/volcengine/verl && cd verl\n\n\nPPO Test\n\nDownload and preprocess the GSM8K dataset:\n\npython3 examples/data_preprocess/gsm8k.py --local_dir ~/data/gsm8k\n\nDownload pretrained model weights:\n\npython3 -c \"import transformers; transformers.pipeline('text-generation', model='Qwen/Qwen2.5-0.5B-Instruct')\"\n\nRun PPO training with a rule-based reward model:\n\nPYTHONUNBUFFERED=1 python3 -m verl.trainer.main_ppo \\\n data.train_files=$HOME/data/gsm8k/train.parquet \\\n data.val_files=$HOME/data/gsm8k/test.parquet \\\n data.train_batch_size=256 \\\n data.max_prompt_length=512 \\\n data.max_response_length=256 \\\n actor_rollout_ref.model.path=Qwen/Qwen2.5-0.5B-Instruct \\\n actor_rollout_ref.actor.optim.lr=1e-6 \\\n actor_rollout_ref.actor.ppo_mini_batch_size=64 \\\n actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4 \\\n actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8 \\\n actor_rollout_ref.rollout.tensor_model_parallel_size=1 \\\n actor_rollout_ref.rollout.gpu_memory_utilization=0.4 \\\n actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4 \\\n critic.optim.lr=1e-5 \\\n critic.model.path=Qwen/Qwen2.5-0.5B-Instruct \\\n critic.ppo_micro_batch_size_per_gpu=4 \\\n algorithm.kl_ctrl.kl_coef=0.001 \\\n trainer.logger=['console'] \\\n trainer.val_before_train=False \\\n trainer.default_hdfs_dir=null \\\n trainer.n_gpus_per_node=1 \\\n trainer.nnodes=1 \\\n trainer.save_freq=10 \\\n trainer.test_freq=10 \\\n trainer.total_epochs=15 2&gt;&1 | tee verl_demo.log\n\n\nExperimental Results\nThe experiment successfully ran with a single-card PPO, with approximately 20GB of GPU memory usage. Next, I will attempt to reproduce DAPO and GRPO, and promptly review literature on custom reward functions to apply new ideas to DAPO/GRPO."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wang's Blog",
    "section": "",
    "text": "M.Eng. at Dalian University of Technology (DUTIR Lab)\nIncoming Ph.D.¬†Student at University of Tokyo (Echizen Lab)\nResearching Multimodal Toxicity Detection, LLM, and Responsible AI"
  },
  {
    "objectID": "index.html#hi-i-am-hongbo-wang",
    "href": "index.html#hi-i-am-hongbo-wang",
    "title": "Wang's Blog",
    "section": "",
    "text": "M.Eng. at Dalian University of Technology (DUTIR Lab)\nIncoming Ph.D.¬†Student at University of Tokyo (Echizen Lab)\nResearching Multimodal Toxicity Detection, LLM, and Responsible AI"
  },
  {
    "objectID": "index.html#research-motivation",
    "href": "index.html#research-motivation",
    "title": "Wang's Blog",
    "section": "Research Motivation",
    "text": "Research Motivation\nSince 2022, I have been dedicated to researching various forms of toxic speech, including hate speech, offensive language, and patronizing and condescending language (PCL). This line of research includes the construction of a PCL dataset (NLPCC‚Äô23), an LLM-based PCL detector (EMNLP‚Äô24), and the multimodal PCL detection (ICASSP‚Äô25).\nMy research is driven by the goal of protecting vulnerable groups from toxic online discourse and investigating novel cybersecurity paradigms in the context of LLMs/MLLMs."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Wang's Blog",
    "section": "Education",
    "text": "Education\n\nPh.D.¬†in Information and Communication Engineering, University of Tokyo, 2025 -\n\nM.Eng. in Computer Science and Technology, Dalian University of Technology, 2022 - 2025\n\nB.Eng. in Computer Science and Technology, Dalian University of Technology, 2017 - 2022"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Wang's Blog",
    "section": "Publications",
    "text": "Publications\n\nCCPC: A Hierarchical Chinese Corpus for Patronizing and Condescending Language Detection\nNLPCC 2023\nHongbo Wang, Mingda Li, Junyu Lu, Liang Yang, Hebin Xia, Hongfei Lin\n[PDF] [Code]\nPclGPT: A Large Language Model for Patronizing and Condescending Language Detection\nEMNLP 2024 (Findings)\nHongbo Wang, Mingda Li, Junyu Lu, Hebin Xia, Liang Yang, Bo Xu, Ruizhu Liu, Hongfei Lin\n[PDF] [Code]\nTowards Patronizing and Condescending Language in Chinese Videos: A Multimodal Dataset and Detector\nICASSP 2025\nHongbo Wang, Junyu Lu, Yan Han, Kai Ma, Liang Yang, Hongfei Lin\n[PDF] [Code]\nGuts at SemEval-2022 Task 4: Adversarial Training and Balancing Methods for Patronizing and Condescending Language Detection\nSemEval-2022\nJunyu Lu, Hao Zhang, Tongyue Zhang, Hongbo Wang, Haohao Zhu, Bo Xu, Hongfei Lin\n[PDF]\nTowards Comprehensive Detection of Chinese Harmful Memes\nNeurIPS 2024\nJunyu Lu, Bo Xu, Xiaokun Zhang, Hongbo Wang, Haohao Zhu, Dongyu Zhang, Liang Yang, Hongfei Lin\n[PDF] [Code]"
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "Wang's Blog",
    "section": "Skills",
    "text": "Skills\n\n\nPython\n\n\nDeep Learning\n\n\nLinguistic Analysis"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Wang's Blog",
    "section": "Contact",
    "text": "Contact\nüìß dutlaowang@mail.dlut.edu.cn\n Google Scholar |  GitHub |  Xiaohongshu"
  },
  {
    "objectID": "paper2.html",
    "href": "paper2.html",
    "title": "Note 2 - Bellman optimality equation (Section 3)",
    "section": "",
    "text": "‚¨Ö Back\n\nSection 3\nÂ§ç‰π†‰πãÂâçÁöÑÂ∑•‰Ωú State valueÊàëËÆ§‰∏∫ÊòØ‰∏ÄÁßçÂÖ®Â±ÄÁöÑ‰ª∑ÂÄºÔºåËÄåaction valueÂÜ≥ÂÆö‰∫ÜÊàë‰ª¨ÁöÑÂÖ∑‰ΩìÁ≠ñÁï•ÈÄâÊã©„ÄÇ\nË¥ùÂ∞îÊõºÂÖ¨ÂºèÂ∞±ÊòØ‰∏∫‰∫ÜËÆ°ÁÆóstate value„ÄÇ\nÊàë‰ª¨ÂÖàÈÄöËøáË¥ùÂ∞îÊõºÁü©ÈòµÂºèËÆ°ÁÆóÂá∫state value,ÂÜçÂà©Áî®ÂÆö‰πâÂºèÊãÜÂàÜÂá∫ÂØπÂ∫îÁöÑaction value\n\nBellman Optimal Equation\n\nImage source: Shiyu Zhao, Westlake University\n\nÈáçË¶ÅÂå∫Âà´\nË¥ùÂ∞îÊõºÂÖ¨ÂºèËÆ°ÁÆóÁöÑÊòØ ÁªôÂÆöÁ≠ñÁï•ÁöÑstate value\nÊúÄ‰ºòË¥ùÂ∞îÊõºËÆ°ÁÆóÁöÑÊòØ ÊúÄ‰ºòÁ≠ñÁï•‰∏ãÁöÑstate value\nÊôÆÈÄöË¥ùÂ∞îÊõºÂÖ¨ÂºèÁªôÂá∫ÁöÑ state value ÂèçÊò†‰∫ÜÂú®ÊüêÁ≠ñÁï•‰∏ã‰ªéÊüêÁä∂ÊÄÅÂá∫ÂèëÁöÑÈ¢ÑÊúüÂõûÊä•Ôºå‰ΩÜÂÆÉÂπ∂‰∏ç‰øùËØÅÊòØÊúÄ‰ºòÁ≠ñÁï•„ÄÇ\nÊúÄ‰ºòË¥ùÂ∞îÊõºÂÖ¨ÂºèÁöÑÊÑè‰πâÂú®‰∫éÔºåÈÄöËøáÈÄíÂΩíËÆ°ÁÆóÊúÄ‰ºòÁöÑ state value Êàñ action valueÔºåÂ∏ÆÂä©ÊâæÂà∞ËÉΩÊúÄÂ§ßÂåñÂõûÊä•ÁöÑÁ≠ñÁï•„ÄÇÂÆÉÈÄöËøáÈÄâÊã©ÊúÄÂ§ßÂõûÊä•ÁöÑÂä®‰ΩúÔºåÊúÄ‰ºòË¥ùÂ∞îÊõºÁöÑaction valueÊòØÊé®ÁÆóÂá∫Êù•ÁöÑÔºå‰∏îÊòØÂîØ‰∏ÄÊúÄ‰ºòÁöÑÔºÅÂ¶Ç‰∏ãÂõæÔºåÂ∑¶ËæπÊòØÁªèBOEÂæóÂà∞ÁöÑÊúÄ‰ºòaction,Âè≥ËæπÊòØactionÂØπÂ∫îÁöÑÂÖ®Â±Ästate value„ÄÇËøôÊòØ‰∏Ä‰∏™Ëá™Âä®‰ºòÂåñÂæóÂà∞ÁöÑËøáÁ®ã„ÄÇ\n\nImage source: Shiyu Zhao, Westlake University"
  },
  {
    "objectID": "paper4.html",
    "href": "paper4.html",
    "title": "Study Notes",
    "section": "",
    "text": "Reinforcement LearningMultimodal PCL\n\n\n\n\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.plot(np.random.randn(100).cumsum())\nplt.title(\"RL Noise Simulation\")\nplt.show()\n\nConclusion: RL model adapts well under noisy settings.\n\n\n\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nrewards = np.linspace(0, 10, 100)\nshaped = rewards ** 0.5\nprint(\"Shaped Rewards:\", shaped[:5])\n\nConclusion: Adaptive reward shaping speeds up training.\n\n\n\n\n\nMultimodal PCL detection using video and text features.\n\ntext_features = [0.8, 0.9, 0.85]\nvideo_features = [0.7, 0.75, 0.78]\ncombined = [ (t+v)/2 for t, v in zip(text_features, video_features)]\nprint(\"Combined Features:\", combined)"
  },
  {
    "objectID": "paper4.html#research-updates",
    "href": "paper4.html#research-updates",
    "title": "Study Notes",
    "section": "",
    "text": "Reinforcement LearningMultimodal PCL\n\n\n\n\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.plot(np.random.randn(100).cumsum())\nplt.title(\"RL Noise Simulation\")\nplt.show()\n\nConclusion: RL model adapts well under noisy settings.\n\n\n\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nrewards = np.linspace(0, 10, 100)\nshaped = rewards ** 0.5\nprint(\"Shaped Rewards:\", shaped[:5])\n\nConclusion: Adaptive reward shaping speeds up training.\n\n\n\n\n\nMultimodal PCL detection using video and text features.\n\ntext_features = [0.8, 0.9, 0.85]\nvideo_features = [0.7, 0.75, 0.78]\ncombined = [ (t+v)/2 for t, v in zip(text_features, video_features)]\nprint(\"Combined Features:\", combined)"
  },
  {
    "objectID": "paper4.html#research-updates-1",
    "href": "paper4.html#research-updates-1",
    "title": "Study Notes",
    "section": "Research Updates",
    "text": "Research Updates\n\nReinforcement LearningMultimodal PCL\n\n\n\nPaper 1 - State Action Policy (Noise Simulation)\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.plot(np.random.randn(100).cumsum())\nplt.title(\"RL Noise Simulation\")\nplt.show()\n\nConclusion: RL model adapts well under noisy settings.\n\n\nPaper 2 - State Action Policy (Reward Shaping)\nReinforcement learning has made great progress in noisy environments.\n\nimport numpy as np\nrewards = np.linspace(0, 10, 100)\nshaped = rewards ** 0.5\nprint(\"Shaped Rewards:\", shaped[:5])\n\nConclusion: Adaptive reward shaping speeds up training.\n\n\n\n\nPaper 3 - Multimodal PCL Detection\nMultimodal PCL detection using video and text features.\n\ntext_features = [0.8, 0.9, 0.85]\nvideo_features = [0.7, 0.75, 0.78]\ncombined = [ (t+v)/2 for t, v in zip(text_features, video_features)]\nprint(\"Combined Features:\", combined)\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.plot(np.random.randn(100).cumsum())\nplt.title(\"RL Noise Simulation\")\nplt.show()"
  },
  {
    "objectID": "ppt.html",
    "href": "ppt.html",
    "title": "My PPT Showcase (Click to view PDFs)",
    "section": "",
    "text": "üîî Updates: - 2025-05-02: Added GRPO_DAPO RL PPT"
  }
]